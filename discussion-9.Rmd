---
title: 'BIOST 537: Discussion 9'
author: "Marlena Bannick"
date: "2/23/2021"
output:
  beamer_presentation: default
  html_document:
    theme: yeti
  ioslides_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Learning Objectives

- Continue with our review of the main methods in the course, and how they fit together, with the addition of time-varying covariates
- Answer any outstanding questions about the main topics in the course
- Answer questions about the group project (if there are any)

Like last time, these slides are going to be focused on the concepts rather than mathematical details.
Hard stop at 1:00 (hold me to it!) so that we can dive into questions for ~20 min.

## Slides

- Slides are available on Canvas
- RMarkdown can be checked out via GitHub

```{sh, eval=F}
git clone
 https://github.com/mbannick/survival-discussion-section.git
```

Or just `git pull` if you already have it cloned!

## Data

```{r}
library(survival)
library(flexsurv)
source("fitparametric.R")
data("diabetic")
head(diabetic)
```

## Data

The "diabetic" dataset from package `survival`

- `id` subject id
- `laser` laser type: xenon or argon
- `age` age at diagnosis
- `eye` a factor with levels of left right
- `trt` treatment: 0 = no treatment, 1= laser
- `risk` risk group of 6-12
- `time` time to event or last follow-up
- `status` status of 0 = censored or 1 = visual loss

## Estimate a survival function

If we have some survival times, potentially with censoring, how can we estimate a survival function?

- *Parametric*: specify some distribution that you think the survival times follow (e.g. Weibull) and use maximum likelihood estimation to estimate the parameters of that distribution
- *Non-parametric*: use the Kaplan-Meier ("product limit" estimator) which is the product of the hazard at each observed event time

## Estimate a cumulative hazard function

If we have some survival times, potentially with censoring, how can we estimate a cumulative hazard function?

- *Parametric*: We know how the cumulative hazard function and the survival function are related, and we know the formula for the survival function (since we picked a particular distribution) so we take our estimated parameters from the parametric distribution and get the survival function, then take the negative log.

$$
H(t) = -\log S(t)
$$

- *Non-parametric*: use the Nelson-Aalen estimator which is the sum of the hazards at each observed event time

## Estimate a hazard function

If we have some survival times, potentially with censoring, how can we estimate a hazard function?

- *Parametric*: Again, we know the specific parametric form because we chose it! And we know that:

$$
h(t) = \frac{f(t)}{S(t)}
$$

- *Non-parametric*: use the Nelson-Aalen estimator, but then smooth over the differences in hazard between adjacent time points with kernel density estimation

## Test for differences between two groups

- *Parametric*: A few options!
  - Fit two parametric models and perform a Wald test to test for differences in parameters
  - Fit one parametric model with binary covariate on the hazard and perform Wald test on the $\beta$ coefficient representing the log hazard ratio
  - Fit a parametric model with no covariates, and a parametric model with the one binary covariate and compare the two fits with a score test or likelihood ratio test
  - For the last two approaches, could also include a confounding variable in the models.

## Test for differences between two groups

- *Non-parametric*:
  - The logrank test (and all of its variants based on different weighting schemes)
  - Stratified logrank test when there are confounding variables

## Estimate a hazard ratio

- *Parametric*: Proportional hazards model. We just did this! Estimate a parameter for the baseline hazard, and then coefficients that represent the difference in log hazard between unit differences of the predictors. $\exp(\beta)$ is a hazard ratio. All using maximum likelihood estimation. E.g. exponential:

$$
h(t) = h_0(t) \exp(\beta_1 x_1 + \cdots + \beta_p x_p) \quad h_0(t) = \lambda
$$

By including multiple $x$, the interpretation of the $\beta$ parameters is the hazard ratio comparing two groups, but where all other $x$ are held constant.

## Estimate a hazard ratio

- *Semi-parametric*: **Cox** proportional hazards model. Do basically the same thing, but now $h_0(t)$ is **unspecified** and a nuisance! So we don't have to make an assumption about the parametric form of survival times (e.g. exponential).

We can also include multiple $x$ here to adjust for confounding. OR we can use stratified Cox proportional hazards, where each group gets its own baseline hazard that we don't estimate (nuisance parameters).

**NEW**: What about if we have time-varying covariates?

## Time-Varying Covariates

It's conceivable that more than just a subjects' baseline covariates impacts their risk of failure.
Maybe we want to:

- Properly account for a confounding variable that varies over time
- Understand the relationship between a variable and survival, where the variable varies over time

Consider covariates that look like $z_i(t)$ rather than $z_i$ for the $i^{th}$ subject.

*Note: This is not a time-varying effect. If we wanted that we'd use the notation $\beta(t)$.*

## Time-Varying Covariates

Recall when we covered the Cox PH model, and the stratified Cox PH model, the way that we estimated the parameters was with the partial likelihood.

In the partial likelihood:
- We compared *ranked* failure times rather than actual failure times
- The comparisons were made against other individuals in the risk set. For stratified Cox, we stratified the risk sets.

## Estimate a hazard ratio

Using `coxph` to semi-parametrically estimate a hazard ratio, we would do:

```{r}
coxph(diab.surv ~ trt, data=diabetic)
```

Note that we now have nothing that tells us about the baseline hazard. We didn't estimate it!

## Estimate a hazard ratio

What about controlling for a confounder?

```{r}
coxph(diab.surv ~ trt + risk, data=diabetic)
```

## Estimate a hazard ratio

What about more **flexibly** controlling for a confounder?

```{r}
coxph(diab.surv ~ trt + strata(risk), data=diabetic)
```

## Estimate acceleration factors on time scale

We've got only one option here, and it's parametric. You need to assume some parametric form for your data.

Then, we can fit an accelerated failure time model, where $A(x) = \exp(\phi_1 x_1 + \cdots + \phi_p x_p)$ is your acceleration factor.

$$
S(t | x) = S_0(A(x) t)
$$

You get an "implied density" from this if you substituted $A(x) t$ in for $t$ in the typical functions (density, survival, hazard, etc.) for whatever parametric model you're working with.

Note, if we parameterized $A^*(x) = \exp(-\phi_1 x_1 - \cdots + \phi_p x_p)$, then $A(x) = 1/A^*(x)$.

## Estimating acceleration factors on time scale

We want to estimate the $\phi$ parameters in the acceleration factor. It turns out that for the Weibull parameterization there is a direct relationship between the HR and acceleration factor.

```{r}
mod <- flexsurvreg(Surv(time, status) ~ trt, 
                   data=diabetic, dist="weibull")
mod$res
```

## Estimating acceleration factors on time scale

If we had used `dist="exponential"`, the `flexsurvreg` function would have returned a hazard ratio.
With `dist="weibull"` it returns an average increase in survival time (1/acceleration factor).

```{r}
exp(mod$res["trt", "est"])
1 / exp(mod$res["trt", "est"])
```

Translates to average survival time in the treated group being 1/0.377 = 2.65 times as long as the untreated group.

## Understanding Check

<iframe src="https://pollev-embeds.com/mnorwood" width="800px" height="600px"></iframe>

## Understanding Check

<iframe src="https://embed.polleverywhere.com/clickable_images/p6IWqrBo0jEYFX4BmFwBe?controls=none&short_poll=true" width="800px" height="600px"></iframe>

