---
title: 'BIOST 537: Discussion 9: Main Concepts + Time-Varying Covariates'
author: "Marlena Bannick"
date: "2/23/2021"
output:
  ioslides_presentation: default
  html_document:
    theme: yeti
  slidy_presentation: default
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Learning Objectives

- Continue with our review of the main methods in the course, and how they fit together, with the addition of time-varying covariates
- Answer any outstanding questions about the main topics in the course
- Answer questions about the group project (if there are any)

Like last time, these slides are going to be focused on the concepts rather than mathematical details.
Hard stop at 1:00 (hold me to it!) so that we can dive into questions for ~20 min.

## Slides

- Slides are available on Canvas
- RMarkdown can be checked out via GitHub

```{sh, eval=F}
git clone
 https://github.com/mbannick/survival-discussion-section.git
```

Or just `git pull` if you already have it cloned!

## Data

```{r}
library(data.table)
library(survival)
library(flexsurv)
source("fitparametric.R")
data("diabetic")
diabetic <- data.table(diabetic)
head(diabetic)
```

## Data

The "diabetic" dataset from package `survival`

- `id` subject id
- `laser` laser type: xenon or argon
- `age` age at diagnosis
- `eye` a factor with levels of left right
- `trt` treatment: 0 = no treatment, 1= laser
- `risk` risk group of 6-12
- `time` time to event or last follow-up
- `status` status of 0 = censored or 1 = visual loss

## Estimate a survival function

If we have some survival times, potentially with censoring, how can we estimate a survival function?

- *Parametric*: specify some distribution that you think the survival times follow (e.g. Weibull) and use maximum likelihood estimation to estimate the parameters of that distribution
- *Non-parametric*: use the Kaplan-Meier ("product limit" estimator) which is the product of the hazard at each observed event time

## Estimate a cumulative hazard function

If we have some survival times, potentially with censoring, how can we estimate a cumulative hazard function?

- *Parametric*: We know how the cumulative hazard function and the survival function are related, and we know the formula for the survival function (since we picked a particular distribution) so we take our estimated parameters from the parametric distribution and get the survival function, then take the negative log.

$$
H(t) = -\log S(t)
$$

- *Non-parametric*: use the Nelson-Aalen estimator which is the sum of the hazards at each observed event time

## Estimate a hazard function

If we have some survival times, potentially with censoring, how can we estimate a hazard function?

- *Parametric*: Again, we know the specific parametric form because we chose it! And we know that:

$$
h(t) = \frac{f(t)}{S(t)}
$$

- *Non-parametric*: use the Nelson-Aalen estimator, but then smooth over the differences in hazard between adjacent time points with kernel density estimation

## Test for differences between two groups

- *Parametric*: A few options!
  - Fit two parametric models and perform a Wald test to test for differences in parameters
  - Fit one parametric model with binary covariate on the hazard and perform Wald test on the $\beta$ coefficient representing the log hazard ratio
  - Fit a parametric model with no covariates, and a parametric model with the one binary covariate and compare the two fits with a score test or likelihood ratio test
  - For the last two approaches, could also include a confounding variable in the models.

## Test for differences between two groups

- *Non-parametric*:
  - The logrank test (and all of its variants based on different weighting schemes)
  - Stratified logrank test when there are confounding variables

## Estimate a hazard ratio

- *Parametric*: Proportional hazards model. We just did this! Estimate a parameter for the baseline hazard, and then coefficients that represent the difference in log hazard between unit differences of the predictors. $\exp(\beta)$ is a hazard ratio. All using maximum likelihood estimation. E.g. exponential:

$$
h(t) = h_0(t) \exp(\beta_1 x_1 + \cdots + \beta_p x_p) \quad h_0(t) = \lambda
$$

By including multiple $x$, the interpretation of the $\beta$ parameters is the hazard ratio comparing two groups, but where all other $x$ are held constant.

## Estimate a hazard ratio

- *Semi-parametric*: **Cox** proportional hazards model. Do basically the same thing, but now $h_0(t)$ is **unspecified** and a nuisance! So we don't have to make an assumption about the parametric form of survival times (e.g. exponential).

We can also include multiple $x$ here to adjust for confounding. OR we can use stratified Cox proportional hazards, where each group gets its own baseline hazard that we don't estimate (nuisance parameters).

**NEW**: What about if we have time-varying covariates?

## Cox PH

Let's fit a Cox PH model to understand the relationship between treatment and survival, controlling for risk level. Let's focus on just one eye.

```{r}
df <- diabetic[eye == "left"]
diab.surv <- Surv(time=df$time,
                  event=df$status)
coxph(diab.surv ~ trt + risk, data=df)
```

## Time-Varying Covariates

It's conceivable that more than just a subject's baseline covariates impacts their risk of failure.
Maybe we want to:

- Properly account for a confounding variable that varies over time
- Understand the relationship between a variable and survival, where the variable varies over time

Consider covariates that look like $z_i(t)$ rather than $z_i$ for the $i^{th}$ subject.

*Note: This is not a time-varying effect. If we wanted that we'd use the notation $\beta(t)$.*

## Time-Varying Covariates

Recall when we covered the Cox PH model, and the stratified Cox PH model, how we estimated the parameters the partial likelihood approach.

In the partial likelihood:

- We compared *ranked* failure times rather than actual failure times
- The comparisons were made against other individuals in the risk set. For stratified Cox, we stratified the risk sets.
- The comparisons were made using only the baseline covariates.

## Time-Varying Covariates

In the time-varying covariates scenario, failure times are still not included in the model in any way, but(!!) we need to use those failure times to extract the correct covariate values $z_i(t)$ for each subject that is currently in the risk set.

More of a data manipulation exercise than drastically changing the model. Which makes sense in terms of what we have to do as users of this method: we need to get our data into the correct structure using `tmerge`.

## Time-Varying Covariates

For the data manipulations in class, we used the `tmerge` function take a data set that already had a time that signified the change point of something happening.

We're going to modify the diabetic data set with some fake changes in a different way than `tmerge` to understand the mechanics of what's happening.

## Time-Varying Covariates

```{r}
# Create a new diabetic dataset
# And only take one eye, they were randomized
df <- diabetic[eye == "left"]
new.df <- data.table()
```

## Time-Varying Covariates

```{r, eval=F}
for(i in unique(df$id)){
  # Subset to only this person
  sub <- df[id == i]
  
  # What is this person's risk level and failure time
  risk <- sub$risk; time <- sub$time
  
  # Calculate the time that the risk level will change
  change.time <- time/sample(2:10, 1)
  
  # Decide whether their risk will go up or down
  bin <- rbinom(1, size=1, prob=0.5)
  increase <- ifelse(bin, 1, -1)
```

## Time-Varying Covariates

```{r, eval=FALSE}
  # Modify existing entry
  sub[, tstart := 0]
  sub[, tstop := change.time]
  
  # Duplicate the entry
  sub2 <- copy(sub)
  sub2[, tstart := change.time]
  sub2[, tstop := time]
  sub2[, risk := risk + increase]
  
  # Append to data frame
  new.df <- rbind(new.df, sub)
  new.df <- rbind(new.df, sub2)
}
```

```{r, eval=TRUE, echo=FALSE}
for(i in unique(df$id)){
  # Subset to only this person
  sub <- df[id == i]
  
  # What is this person's risk level and failure time
  risk <- sub$risk; time <- sub$time
  
  # Calculate the time that the risk level will change
  change.time <- time/sample(2:10, 1)
  
  # Decide whether their risk will go up or down
  bin <- rbinom(1, size=1, prob=0.5)
  increase <- ifelse(bin, 1, -1)
  
  # Modify existing entry
  sub[, tstart := 0]
  sub[, tstop := change.time]
  
  # Duplicate the entry
  sub2 <- copy(sub)
  sub2[, tstart := change.time]
  sub2[, tstop := time]
  sub2[, risk := risk + increase]
  
  # Append to data frame
  new.df <- rbind(new.df, sub)
  new.df <- rbind(new.df, sub2)
}
```

## Time-Varying Covariates

```{r}
head(df)
```

```{r}
head(new.df)
```

## Estimate a hazard ratio with time-varying covariates

Now we can use `coxph` just like we normally would, but we have our augmented dataset with two rows per individual! We still get one coefficient for risk, but it's taking into account that the risk values differ for different people over time.

Also, note that because of how we made `tstart` and `tstop`, the same individual will never be in the risk set twice (it's almost like you're a new individual that had delayed entry!).

## Estimate a hazard ratio with time-varying covariates

```{r}
diab.surv <- Surv(time=new.df$tstart,
                  time2=new.df$tstop,
                  event=new.df$status)
coxph(diab.surv ~ trt + risk, data=new.df)
```

Note that we now have nothing that tells us about the baseline hazard. We didn't estimate it!

## Estimate acceleration factors on time scale

We've got only one option here, and it's parametric. You need to assume some parametric form for your data.

Then, we can fit an accelerated failure time model, where $A(x) = \exp(\phi_1 x_1 + \cdots + \phi_p x_p)$ is your acceleration factor.

$$
S(t | x) = S_0(A(x) t)
$$

You get an "implied density" from this if you substituted $A(x) t$ in for $t$ in the typical functions (density, survival, hazard, etc.) for whatever parametric model you're working with.

Note, if we parameterized $A^*(x) = \exp(-\phi_1 x_1 - \cdots + \phi_p x_p)$, then $A(x) = 1/A^*(x)$.

## Estimating acceleration factors on time scale

We want to estimate the $\phi$ parameters in the acceleration factor. It turns out that for the Weibull parameterization there is a direct relationship between the HR and acceleration factor.

```{r}
mod <- flexsurvreg(Surv(time, status) ~ trt, 
                   data=diabetic, dist="weibull")
mod$res
```

## Estimating acceleration factors on time scale

If we had used `dist="exponential"`, the `flexsurvreg` function would have returned a hazard ratio.
With `dist="weibull"` it returns an average increase in survival time (1/acceleration factor).

```{r}
exp(mod$res["trt", "est"])
1 / exp(mod$res["trt", "est"])
```

Translates to average survival time in the treated group being 1/0.377 = 2.65 times as long as the untreated group.
